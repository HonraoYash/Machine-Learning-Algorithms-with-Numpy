{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "    \n",
    "class Transformer:\n",
    "    def __init__(self, context_length, embedding_dim, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_length = context_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.w_q = np.random.randn(embedding_dim, 64)\n",
    "        self.w_k = np.random.randn(embedding_dim, 64)\n",
    "        self.w_v = np.random.randn(embedding_dim, 64)\n",
    "\n",
    "    def softmax(self, w):\n",
    "        exp_w = np.exp(w = np.max(w, axis=1, keepdims=True))\n",
    "        return exp_w / np.sum(exp_w, axis=1, keepdims=True)    \n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def attention_output(self,X):\n",
    "        # X is the text embeddings (m,embedding_size)\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "\n",
    "        # Calculating the Queries, Keys, Values vectors\n",
    "        Q = X @ self.w_q     \n",
    "        K = X @ self.w_k\n",
    "        V = X @ self.w_v             # (m, emb) @ (emb, 64) = (m, 64)\n",
    " \n",
    "        # Computing the attention scores\n",
    "        attention_scores = Q @ K.T   # (m,m)\n",
    "\n",
    "        # Normalizing the attention scores\n",
    "        normalized_attention_scores = attention_scores / 8                 # Because the dimension size of K is 64 # (m, m)\n",
    "\n",
    "        # Calculating the attention weights\n",
    "        attention_weights = self.softmax(normalized_attention_scores)      # (m, m)\n",
    "\n",
    "        # Weighted sum of the weights\n",
    "        weighted_sum = attention_weights @ V      # (m, m) @ (m, 64) = (m, 64)\n",
    "\n",
    "        return weighted_sum  \n",
    "    \n",
    "    def cross_attention(self, X1, X2):\n",
    "        Q = X2 @ self.w_q\n",
    "        K = X1 @ self.w_k\n",
    "        V = X1 @ self.w_v\n",
    "\n",
    "        # Computing the attention scores\n",
    "        attention_scores = Q @ K.T   # (m,m)\n",
    "\n",
    "        # Normalizing the attention scores\n",
    "        normalized_attention_scores = attention_scores / 8                 # Because the dimension size of K is 64 # (m, m)\n",
    "\n",
    "        # Calculating the attention weights\n",
    "        attention_weights = self.softmax(normalized_attention_scores)      # (m, m)\n",
    "\n",
    "        # Weighted sum of the weights\n",
    "        weighted_sum = attention_weights @ V      # (m, m) @ (m, 64) = (m, 64)\n",
    "\n",
    "        return weighted_sum\n",
    "    \n",
    "    def feed_forward(self, X, input_dim, hidden_dim, output_dim, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.w1 = np.random.randn(input_dim, hidden_dim)\n",
    "        self.b1 = np.zeros((1, input_dim))\n",
    "        self.w2 = np.random.randn(hidden_dim, output_dim)\n",
    "        self.b2 = np.zeros((1, output_dim))\n",
    "\n",
    "        z1 = X @ self.w1 + self.b1\n",
    "        a1 = self.relu(z1)\n",
    "        z2 = a1 @ self.w2 + self.b2\n",
    "        a2 = z2\n",
    "\n",
    "        return a2\n",
    "    \n",
    "    def linear(self, X):\n",
    "        w1 = np.random.randn(X.shape[1], self.vocab_size)\n",
    "        linear_out = X @ w1\n",
    "        return linear_out\n",
    "\n",
    "    def layer_norm(self,X):\n",
    "        mean = np.mean(X)\n",
    "        std = (X - np.mean(X))/X.shape[0]\n",
    "        X = (X - mean) / std\n",
    "\n",
    "    def encoder(self, X):\n",
    "        # Get the output of the self-attention\n",
    "        attention_out = self.attention_output(X)         # (context_length, embedding_dim)\n",
    "\n",
    "        # Pass these outputs parallely to the Feed-forward Neural Network\n",
    "        ffnn_out = self.feed_forward(self.layer_norm(attention_out + X), self.embedding_dim, 4*self.embedding_dim, self.embedding_dim, learning_rate=0.01)\n",
    "\n",
    "        encoder_out = self.layer_norm(ffnn_out + self.layer_norm(attention_out + X))\n",
    "\n",
    "        return encoder_out\n",
    "    \n",
    "    def decoder(self, encoder_out, previous_token): \n",
    "        masked_attention_out = self.attention_output(previous_token)         # Assuming same weights for both the encoder and decoder self-attention layer, but ideally it should be different \n",
    "\n",
    "        cross_attention_out = self.cross_attention(encoder_out, self.layer_norm(masked_attention_out + previous_token))  # Assuming the same weights of self-attention layer for cross-attention layer\n",
    "\n",
    "        ffnn_out = self.feed_forward(self.layer_norm(cross_attention_out + self.layer_norm(masked_attention_out + previous_token)))\n",
    "\n",
    "        decoder_out = self.layer_norm(ffnn_out + cross_attention_out) \n",
    "\n",
    "        linear_out = self.linear(decoder_out)\n",
    "\n",
    "        probabilities = self.softmax(linear_out)\n",
    "\n",
    "        predicted_word = np.argmax(probabilities)  \n",
    "\n",
    "        previous_token = predicted_word\n",
    "\n",
    "        return predicted_word\n",
    "    \n",
    "    def predict(self, seq):\n",
    "        encoder_out = self.encoder(seq)\n",
    "        decoder_out = self.decoder(encoder_out, '<end>')\n",
    "\n",
    "sequence = np.random.randn(1000, 512)    # embeddings\n",
    "# positional_embeddings = self.add_positional_embeddings(sequence)\n",
    "\n",
    "transformer = Transformer(context_length=1000, embedding_dim=512, vocab_size=10000)\n",
    "predicted_word = transformer.predict(sequence)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
