{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e04a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SelfAttention:\n",
    "    def __init__(self, embedding_size):\n",
    "        self.w_q = np.random.randn(embedding_size, 64)\n",
    "        self.w_k = np.random.randn(embedding_size, 64)\n",
    "        self.w_v = np.random.randn(embedding_size, 64)\n",
    "\n",
    "    def softmax(self,z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=-1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=-1, keepdims=True)  \n",
    "\n",
    "    def attention_output(self, X):\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "\n",
    "        q = X @ self.w_q    # (100,64)\n",
    "        k = X @ self.w_k    # (100,64)\n",
    "        v = X @ self.w_v    # (100,64)\n",
    "\n",
    "        attention_scores = q @ k.T                                       # (100,64) @ (64,100) = (100,100)\n",
    "        normalized_attention_scores = attention_scores / np.sqrt(64)     # (100,1)\n",
    "        attention_weights = self.softmax(normalized_attention_scores)    # (100,1)\n",
    "        weighted_sum = attention_weights @ v                             # (100,100) @ (100,64) = (1,64)\n",
    "\n",
    "        return weighted_sum\n",
    "\n",
    "self_attention_layer = SelfAttention(embedding_size=512)\n",
    "\n",
    "X = np.random.randn(100, 512)\n",
    "attention_output = self_attention_layer.attention_output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd11d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.30778279  10.42221294 -36.50902976 ... -23.80877106  19.16665023\n",
      "  -24.2314227 ]\n",
      " [ -6.3773666   -0.87922991   9.52779687 ...  28.72339927  16.35779464\n",
      "    5.98561188]\n",
      " [-23.0373316   29.06103622  17.40501214 ...  24.24781546  -7.43527149\n",
      "  -19.07495084]\n",
      " ...\n",
      " [-15.37574694 -12.04698284  -1.17155791 ...  28.19857678  24.95907417\n",
      "  -25.05887593]\n",
      " [  4.30778279  10.42221294 -36.50902976 ... -23.80877106  19.16665023\n",
      "  -24.2314227 ]\n",
      " [ 15.66345455  13.39679568   5.6871774  ...  24.70714439  11.84435309\n",
      "   -6.18882984]]\n"
     ]
    }
   ],
   "source": [
    "print(attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30baa33",
   "metadata": {},
   "source": [
    "Below is kind of implementation of the Encoder layer, but it is not thorough so please revise it. Also it does not consider the Add + LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf114f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---- Step 1: Self-Attention Layer ----\n",
    "class SelfAttention:\n",
    "    def __init__(self, embedding_size, d_k=64):\n",
    "        self.d_k = d_k\n",
    "        self.W_q = np.random.randn(embedding_size, d_k) * 0.01\n",
    "        self.W_k = np.random.randn(embedding_size, d_k) * 0.01\n",
    "        self.W_v = np.random.randn(embedding_size, d_k) * 0.01\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=-1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Q = X @ self.W_q\n",
    "        K = X @ self.W_k\n",
    "        V = X @ self.W_v\n",
    "        scores = (Q @ K.T) / np.sqrt(self.d_k)\n",
    "        attn_weights = self.softmax(scores)\n",
    "        out = attn_weights @ V\n",
    "        return out, attn_weights\n",
    "\n",
    "# ---- Step 2: Linear Classifier ----\n",
    "class Classifier:\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        self.W = np.random.randn(input_dim, num_classes) * 0.01\n",
    "        self.b = np.zeros((1, num_classes))\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        logits = X @ self.W + self.b\n",
    "        probs = self.softmax(logits)\n",
    "        return probs, logits\n",
    "\n",
    "# ---- Step 3: Cross-Entropy Loss ----\n",
    "def cross_entropy_loss(probs, y_true):\n",
    "    m = y_true.shape[0]\n",
    "    log_likelihood = -np.log(probs[range(m), y_true])\n",
    "    return np.sum(log_likelihood) / m\n",
    "\n",
    "# ---- Step 4: Dummy Data ----\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(10, 512)     # 10 samples, 512-dim embeddings\n",
    "y = np.random.randint(0, 3, 10)  # 3 classes\n",
    "\n",
    "# ---- Step 5: Forward Pass ----\n",
    "attn = SelfAttention(embedding_size=512)\n",
    "cls = Classifier(input_dim=64, num_classes=3)\n",
    "\n",
    "# Get attention output\n",
    "attn_out, _ = attn.forward(X)          # (10, 64)\n",
    "avg_out = np.mean(attn_out, axis=0, keepdims=True)  # Pool across tokens\n",
    "\n",
    "# Classifier forward\n",
    "probs, logits = cls.forward(avg_out)   # (1, 3)\n",
    "\n",
    "# Compute loss\n",
    "loss = cross_entropy_loss(probs, np.array([y[0]]))\n",
    "print(\"Initial Loss:\", loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
