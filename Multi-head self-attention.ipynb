{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax_stable(x, axis=-1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    expx = np.exp(x)\n",
    "    return expx / np.sum(expx, axis=axis, keepdims=True)\n",
    "\n",
    "class ScaledDotProductAttention:\n",
    "    \"\"\"\n",
    "    Core attention: softmax( Q K^T / sqrt(d_k) ) V\n",
    "    Supports optional mask (e.g., causal or padding).\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.0, seed=42):\n",
    "        self.dropout = dropout\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def __call__(self, Q, K, V, mask=None):\n",
    "        # Q,K,V: (B, H, T, d_k)\n",
    "        d_k = Q.shape[-1]\n",
    "        scores = (Q @ np.swapaxes(K, -1, -2)) / np.sqrt(d_k)   # (B,H,T,T)\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask: broadcastable to (B,H,T,T); masked positions -> -inf\n",
    "            scores = np.where(mask, scores, -1e9)\n",
    "\n",
    "        attn = softmax_stable(scores, axis=-1)                 # (B,H,T,T)\n",
    "\n",
    "        if self.dropout > 0.0:\n",
    "            drop = self.rng.random(attn.shape) >= self.dropout\n",
    "            attn = attn * drop / (1.0 - self.dropout)\n",
    "\n",
    "        out = attn @ V                                         # (B,H,T,d_k)\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention:\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention (from scratch, NumPy).\n",
    "    Shapes:\n",
    "      X: (B, T, d_model)\n",
    "      Output: (B, T, d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.0, seed=42):\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.attn = ScaledDotProductAttention(dropout=dropout, seed=seed)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        # Parameter initialization (Xavier-ish)\n",
    "        k = np.sqrt(1.0 / d_model)\n",
    "        self.W_q = self.rng.uniform(-k, k, size=(d_model, d_model))\n",
    "        self.W_k = self.rng.uniform(-k, k, size=(d_model, d_model))\n",
    "        self.W_v = self.rng.uniform(-k, k, size=(d_model, d_model))\n",
    "        self.W_o = self.rng.uniform(-k, k, size=(d_model, d_model))\n",
    "\n",
    "        # Optional biases (often omitted in attention)\n",
    "        self.b_q = np.zeros((d_model,))\n",
    "        self.b_k = np.zeros((d_model,))\n",
    "        self.b_v = np.zeros((d_model,))\n",
    "        self.b_o = np.zeros((d_model,))\n",
    "\n",
    "    # ----- helpers -----\n",
    "    def _project(self, X, W, b):\n",
    "        # X: (B,T,d_model) -> (B,T,d_model)\n",
    "        return X @ W + b\n",
    "\n",
    "    def _split_heads(self, X):\n",
    "        # (B,T,d_model) -> (B,H,T,d_k)\n",
    "        B, T, _ = X.shape\n",
    "        X = X.reshape(B, T, self.num_heads, self.d_k)\n",
    "        X = np.transpose(X, (0, 2, 1, 3))\n",
    "        return X\n",
    "\n",
    "    def _merge_heads(self, X):\n",
    "        # (B,H,T,d_k) -> (B,T,d_model)\n",
    "        B, H, T, d_k = X.shape\n",
    "        X = np.transpose(X, (0, 2, 1, 3)).reshape(B, T, H * d_k)\n",
    "        return X\n",
    "\n",
    "    # ----- forward -----\n",
    "    def __call__(self, X, mask=None):\n",
    "        \"\"\"\n",
    "        X    : (B, T, d_model)\n",
    "        mask : None or boolean array broadcastable to (B, H, T, T)\n",
    "               True = keep, False = mask-out (set to -inf before softmax)\n",
    "               For causal mask, mask[i,j]=False if j>i.\n",
    "        \"\"\"\n",
    "        B, T, D = X.shape\n",
    "        assert D == self.d_model, \"Bad d_model.\"\n",
    "\n",
    "        # 1) Linear projections\n",
    "        Q = self._project(X, self.W_q, self.b_q)\n",
    "        K = self._project(X, self.W_k, self.b_k)\n",
    "        V = self._project(X, self.W_v, self.b_v)\n",
    "\n",
    "        # 2) Split into heads\n",
    "        Qh, Kh, Vh = self._split_heads(Q), self._split_heads(K), self._split_heads(V)  # (B,H,T,d_k)\n",
    "\n",
    "        # 3) Scaled dot-product attention\n",
    "        out_heads, attn_weights = self.attn(Qh, Kh, Vh, mask=mask)  # (B,H,T,d_k), (B,H,T,T)\n",
    "\n",
    "        # 4) Merge heads and final projection\n",
    "        out = self._merge_heads(out_heads)                          # (B,T,d_model)\n",
    "        out = out @ self.W_o + self.b_o                             # (B,T,d_model)\n",
    "        return out, attn_weights\n",
    "\n",
    "    # ----- masks convenience -----\n",
    "    @staticmethod\n",
    "    def causal_mask(B, T, H):\n",
    "        # shape (B,H,T,T): True for allowed positions i>=j\n",
    "        base = np.tril(np.ones((T, T), dtype=bool))\n",
    "        return np.broadcast_to(base, (B, H, T, T))\n",
    "\n",
    "    @staticmethod\n",
    "    def padding_mask(pad_mask, H):\n",
    "        \"\"\"\n",
    "        pad_mask: (B,T) boolean, True for real tokens, False for pads.\n",
    "        Returns broadcastable mask (B,H,T,T) that blocks attending to pads.\n",
    "        \"\"\"\n",
    "        B, T = pad_mask.shape\n",
    "        # allow attending only to valid keys (dimension K=T)\n",
    "        key_keep = pad_mask[:, None, None, :]        # (B,1,1,T)\n",
    "        # queries always allowed; combine with key_keep\n",
    "        return np.broadcast_to(key_keep, (B, H, T, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e11f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B=2 batches, T=4 tokens, d_model=8, H=2 heads\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(2, 4, 8)\n",
    "\n",
    "mha = MultiHeadSelfAttention(d_model=8, num_heads=2, dropout=0.0)\n",
    "# causal example\n",
    "mask = MultiHeadSelfAttention.causal_mask(B=2, T=4, H=2)\n",
    "Y, A = mha(X, mask=mask)\n",
    "print(\"Output shape:\", Y.shape)        # (2,4,8)\n",
    "print(\"Attention shape:\", A.shape)     # (2,2,4,4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
