{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a91e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TwoLayerNN:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01, epochs=1000):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Initialize weights\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.01\n",
    "        self.b2 = np.zeros((1, output_dim))\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_grad(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # stability\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        eps = 1e-9\n",
    "        return -np.sum(y_true * np.log(y_pred + eps)) / m\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # One-hot encode labels\n",
    "        num_classes = len(np.unique(y))\n",
    "        y_onehot = np.eye(num_classes)[y]\n",
    "\n",
    "        m = X.shape[0]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Forward pass\n",
    "            z1 = X @ self.W1 + self.b1\n",
    "            a1 = self.relu(z1)\n",
    "            z2 = a1 @ self.W2 + self.b2\n",
    "            a2 = self.softmax(z2)\n",
    "\n",
    "            # Loss\n",
    "            loss = self.compute_loss(y_onehot, a2)\n",
    "\n",
    "            # Backpropagation\n",
    "            dz2 = (a2 - y_onehot) / m\n",
    "            dW2 = a1.T @ dz2\n",
    "            db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "            da1 = dz2 @ self.W2.T\n",
    "            dz1 = da1 * self.relu_grad(z1)\n",
    "            dW1 = X.T @ dz1\n",
    "            db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "            # Update\n",
    "            self.W1 -= self.lr * dW1\n",
    "            self.b1 -= self.lr * db1\n",
    "            self.W2 -= self.lr * dW2\n",
    "            self.b2 -= self.lr * db2\n",
    "\n",
    "            # Print progress\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss={loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        z1 = X @ self.W1 + self.b1\n",
    "        a1 = self.relu(z1)\n",
    "        z2 = a1 @ self.W2 + self.b2\n",
    "        probs = self.softmax(z2)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "# Fake dataset: 3 classes, 2 features\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(200, 2)\n",
    "y = np.random.choice(3, 200)\n",
    "\n",
    "model = TwoLayerNN(input_dim=2, hidden_dim=5, output_dim=3, lr=0.1, epochs=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "preds = model.predict(X[:10])\n",
    "print(\"Predictions:\", preds)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a19c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss=1.0986\n",
      "Epoch 100, Loss=1.0985\n",
      "Epoch 200, Loss=1.0984\n",
      "Epoch 300, Loss=1.0980\n",
      "Epoch 400, Loss=1.0971\n",
      "Epoch 500, Loss=1.0945\n",
      "Epoch 600, Loss=1.0892\n",
      "Epoch 700, Loss=1.0830\n",
      "Epoch 800, Loss=1.0786\n",
      "Epoch 900, Loss=1.0752\n",
      "Predictions: [2 0 0 0 0 1 2 0 0 1]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TwoLayerNN_Multi:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.01, epochs=200):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.w1 = np.random.randn(input_dim,hidden_dim)*0.01\n",
    "        self.b1 = np.zeros((1,hidden_dim))\n",
    "        self.w2 = np.random.randn(hidden_dim,output_dim)*0.01\n",
    "        self.b2 = np.zeros((1,output_dim))\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0,z)    \n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z,axis=1,keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def calculate_loss(self, y_pred, y_true):\n",
    "        eps = 1e-9\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred+eps), axis=1))\n",
    "    \n",
    "    def relu_grad(self,z):\n",
    "        return (z > 0).astype(int)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        m = X.shape[0]\n",
    "\n",
    "        num_classes = len(np.unique(y))\n",
    "        y_onehot = np.eye(num_classes)[y]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Forward Pass\n",
    "            z1 = X @ self.w1 + self.b1\n",
    "            a1 = self.relu(z1)    \n",
    "            z2 = a1 @ self.w2 + self.b2\n",
    "            a2 = self.softmax(z2)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = self.calculate_loss(a2, y_onehot)\n",
    "\n",
    "            # Back Propogation\n",
    "            dz2 = (a2 - y_onehot) / m\n",
    "            dw2 = a1.T @ dz2\n",
    "            db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "            da1 = dz2 @ self.w2.T\n",
    "            dz1 = da1 * self.relu_grad(z1)\n",
    "            dw1 = X.T @ dz1\n",
    "            db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "            # Updating the weights\n",
    "            self.w2 -= self.learning_rate * dw2\n",
    "            self.b2 -= self.learning_rate * db2\n",
    "            self.w1 -= self.learning_rate * dw1\n",
    "            self.b1 -= self.learning_rate * db1\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self,X):\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "\n",
    "        z1 = X @ self.w1 + self.b1\n",
    "        a1 = self.relu(z1)    \n",
    "        z2 = a1 @ self.w2 + self.b2\n",
    "        a2 = self.softmax(z2)  \n",
    "\n",
    "        return np.argmax(a2, axis=1)\n",
    "\n",
    "    def score(self,X,y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe407644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 0, 0, 2, 2, 1, 0, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.randn(200,4)\n",
    "y = np.random.choice(3,200)\n",
    "\n",
    "model = TwoLayerNN_Multi(input_dim=4,hidden_dim=5,output_dim=3)\n",
    "model.fit(X,y)\n",
    "\n",
    "model.predict(X[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
