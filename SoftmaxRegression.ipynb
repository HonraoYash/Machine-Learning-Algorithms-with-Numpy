{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb36f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self,learning_rate=0.01,epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.n_classes = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def softmax(self,z):\n",
    "        z = z - np.max(z,axis=1,keepdims=True)\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z,axis=1,keepdims=True)\n",
    "    \n",
    "    def calculate_loss(self,y_pred,y_true):\n",
    "        eps = 1e-9\n",
    "        return -np.mean(np.sum(y_true*np.log(y_pred+eps),axis=1))\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        X = np.asarray(X,dtype=np.float64)\n",
    "        Y = np.asarray(Y,dtype=np.int64)\n",
    "\n",
    "        self.n_classes = len(np.unique(Y))\n",
    "        y_onehot = np.eye(self.n_classes)[Y]\n",
    "\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros((num_features,self.n_classes))  #(n,c)\n",
    "        self.bias = np.zeros((1,self.n_classes))\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            y_pred = self.softmax(X @ self.weights  + self.bias)\n",
    "\n",
    "            dw = (1/num_samples)*(X.T @ (y_pred - y_onehot))\n",
    "            db = (1/num_samples)*np.sum(y_pred - y_onehot,axis=0,keepdims=True)\n",
    "\n",
    "            self.weights -= self.learning_rate*dw\n",
    "            self.bias -= self.learning_rate*db\n",
    "\n",
    "            self.calculate_loss(y_pred,y_onehot)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self,X):\n",
    "        X = np.asarray(X,dtype=np.float64)\n",
    "        probabilities = self.softmax(X @ self.weights + self.bias)\n",
    "        return np.argmax(probabilities,axis=1).astype(int)\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        \"\"\"Compute accuracy.\"\"\"\n",
    "        Y = np.asarray(Y, dtype=np.int64)\n",
    "        preds = self.predict(X)\n",
    "        return np.mean(preds == Y)\n",
    "    \n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(200,4)\n",
    "Y = np.random.choice(3,200)\n",
    "\n",
    "model = SoftmaxRegression(learning_rate=0.01,epochs=1000)\n",
    "model.fit(X,Y)\n",
    "model.predict(X[:20])\n",
    "model.score(X[:20],Y[:20])   \n",
    "\n",
    "# Converting classes to numbers\n",
    "# class_names = df[\"label\"].unique()\n",
    "# class_to_int = {name:i for i,name in enumerate(class_names)}\n",
    "\n",
    "# label_int = df[\"label\"].map(class_to_int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Class Softmax Cross Entropy\n",
    "import numpy as np\n",
    "\n",
    "class SoftmaxRegressionGD:\n",
    "    \"\"\"\n",
    "    Multinomial Logistic Regression (Softmax Regression) using Gradient Descent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, tol=1e-8, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        self.weights = None      # (n_features+1, n_classes)\n",
    "        self.loss_history = []\n",
    "        self.n_classes = None\n",
    "\n",
    "    def add_bias(self, X):\n",
    "        \"\"\"Add bias column (intercept term).\"\"\"\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "    def softmax(self, z):\n",
    "        \"\"\"Row-wise softmax function for probability distribution.\"\"\"\n",
    "        # exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # stability trick\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def calculate_loss(self, X, Y_onehot):\n",
    "        \"\"\"Cross-entropy loss for multi-class classification.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        probs = self.softmax(X @ self.weights)\n",
    "        eps = 1e-9  # prevent log(0)\n",
    "        return -np.sum(Y_onehot * np.log(probs + eps)) / m\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train the softmax regression model using batch gradient descent.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray of shape (n_samples, n_features)\n",
    "        Y : np.ndarray of shape (n_samples,) with integer class labels\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        Y = np.asarray(Y, dtype=np.int64)\n",
    "\n",
    "        # One-hot encode labels\n",
    "        self.n_classes = len(np.unique(Y))\n",
    "        Y_onehot = np.eye(self.n_classes)[Y]   #this creates an Identity matrix with n_classes rows , with 1 in it's diagonal\n",
    "\n",
    "        # Add bias\n",
    "        X = self.add_bias(X)\n",
    "        m, n = X.shape\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights = np.zeros((n, self.n_classes))\n",
    "\n",
    "        prev_loss = float(\"inf\")\n",
    "        for epoch in range(self.epochs):\n",
    "            # Forward pass: probabilities\n",
    "            probs = self.softmax(X @ self.weights)\n",
    "\n",
    "            # Gradient: X.T @ (probs - Y_onehot)\n",
    "            gradients = (X.T @ (probs - Y_onehot)) / m\n",
    "\n",
    "            # Update weights\n",
    "            self.weights -= self.learning_rate * gradients\n",
    "\n",
    "            # Compute and store loss\n",
    "            loss = self.calculate_loss(X, Y_onehot)\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "            # Early stopping\n",
    "            if abs(prev_loss - loss) < self.tol:\n",
    "                if self.verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch}, Loss = {loss:.6f}\")\n",
    "                break\n",
    "            prev_loss = loss\n",
    "\n",
    "            if self.verbose and epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss = {loss:.6f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        X = self.add_bias(X)\n",
    "        return self.softmax(X @ self.weights)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels (argmax over probabilities).\"\"\"\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        \"\"\"Compute accuracy.\"\"\"\n",
    "        Y = np.asarray(Y, dtype=np.int64)\n",
    "        preds = self.predict(X)\n",
    "        return np.mean(preds == Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e6e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataframe\n",
    "df = pd.DataFrame({\n",
    "    \"feature1\": [5.1, 4.9, 6.7, 1.3],\n",
    "    \"feature2\": [3.5, 3.0, 3.1, 1.3],\n",
    "    \"label\": [\"cat\", \"dog\", \"cat\",\"elephant\"]\n",
    "})\n",
    "\n",
    "class_names = df[\"label\"].unique()              # e.g., [\"cat\", \"dog\"]\n",
    "class_to_int = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "Y_int = df[\"label\"].map(class_to_int).values    # convert to numeric\n",
    "print(Y_int)\n",
    "# [0 1 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "935c266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 1.096622\n",
      "Epoch 100, Loss = 1.067714\n",
      "Epoch 200, Loss = 1.067621\n",
      "Early stopping at epoch 228, Loss = 1.067621\n",
      "Predicted labels: [0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0]\n",
      "Predicted probabilities:\n",
      " [[0.4182568  0.23357357 0.34816963]\n",
      " [0.46319455 0.21116189 0.32564355]\n",
      " [0.42373916 0.26340791 0.31285292]\n",
      " [0.42613931 0.18666336 0.38719733]\n",
      " [0.44753514 0.26346265 0.28900221]]\n",
      "Accuracy: 0.3933333333333333\n"
     ]
    }
   ],
   "source": [
    "# Fake dataset: 3 classes, 2 features\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(150, 2)\n",
    "Y = np.random.choice(3, 150)  # labels in {0,1,2}\n",
    "\n",
    "# Train softmax regression\n",
    "model = SoftmaxRegressionGD(learning_rate=0.1, epochs=1000, verbose=True)\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Predictions\n",
    "print(\"Predicted labels:\", model.predict(X[:20]))\n",
    "print(\"Predicted probabilities:\\n\", model.predict_proba(X[:5]))\n",
    "print(\"Accuracy:\", model.score(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3748dd0",
   "metadata": {},
   "source": [
    "Below is a better implementation of Softmax Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c76123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self,learning_rate=0.01,epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.n_classes = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def softmax(self,z):\n",
    "        z = z - np.max(z,axis=1,keepdims=True)\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z,axis=1,keepdims=True)\n",
    "    \n",
    "    def calculate_loss(self,y_pred,y_true):\n",
    "        eps = 1e-9\n",
    "        return -np.mean(np.sum(y_true*np.log(y_pred+eps),axis=1))\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        X = np.asarray(X,dtype=np.float64)\n",
    "        Y = np.asarray(Y,dtype=np.int64)\n",
    "\n",
    "        self.n_classes = len(np.unique(Y))\n",
    "        y_onehot = np.eye(self.n_classes)[Y]\n",
    "\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros((num_features,self.n_classes))  #(n,c)\n",
    "        self.bias = np.zeros((1,self.n_classes))\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            y_pred = self.softmax(X @ self.weights  + self.bias)\n",
    "\n",
    "            dw = (1/num_samples)*(X.T @ (y_pred - y_onehot))\n",
    "            db = (1/num_samples)*np.sum(y_pred - y_onehot,axis=0,keepdims=True)\n",
    "\n",
    "            self.weights -= self.learning_rate*dw\n",
    "            self.bias -= self.learning_rate*db\n",
    "\n",
    "            self.calculate_loss(y_pred,y_onehot)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self,X):\n",
    "        X = np.asarray(X,dtype=np.float64)\n",
    "        probabilities = self.softmax(X @ self.weights + self.bias)\n",
    "        return np.argmax(probabilities,axis=1).astype(int)\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        \"\"\"Compute accuracy.\"\"\"\n",
    "        Y = np.asarray(Y, dtype=np.int64)\n",
    "        preds = self.predict(X)\n",
    "        return np.mean(preds == Y)\n",
    "    \n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(200,4)\n",
    "Y = np.random.choice(3,200)\n",
    "\n",
    "model = SoftmaxRegression(learning_rate=0.01,epochs=1000)\n",
    "model.fit(X,Y)\n",
    "model.predict(X[:20])\n",
    "model.score(X[:20],Y[:20])   \n",
    "\n",
    "# Converting classes to numbers\n",
    "# class_names = df[\"label\"].unique()\n",
    "# class_to_int = {name:i for i,name in enumerate(class_names)}\n",
    "\n",
    "# label_int = df[\"label\"].map(class_to_int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01cbfca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.randn(200,4)\n",
    "Y = np.random.choice(3,200)\n",
    "\n",
    "model = SoftmaxRegression(learning_rate=0.01,epochs=1000)\n",
    "model.fit(X,Y)\n",
    "\n",
    "model.predict(X[:20])\n",
    "\n",
    "model.score(X[:20],Y[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66509e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "     \"feature1\": [5.1, 4.9, 6.7, 1.3],\n",
    "    \"feature2\": [3.5, 3.0, 3.1, 1.3],\n",
    "    \"label\": [\"cat\", \"dog\", \"cat\",\"elephant\"]\n",
    "})\n",
    "\n",
    "class_names = df[\"label\"].unique()\n",
    "class_to_int = {name:i for i,name in enumerate(class_names)}\n",
    "\n",
    "label_int = df[\"label\"].map(class_to_int).values\n",
    "print(label_int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
